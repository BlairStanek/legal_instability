This is the dataset and code corresponding to our paper *LLMs Provide Unstable Answers to Legal Questions*, where we quantify the instability of gpt-4o, claude-3.5, gemini-1.5, and o1 on hard legal questions.  

Our novel dataset of 500 hard legal questions resides as text files in the directory **DATASET**.  For example, the file **DATASET/21_F.3d_1508.in.txt** contains a hard legal question on which gpt-4o, claude-3.5, and gemini-1.5 sometimes return that one party wins, while othertimes returning that the other party wins. The body of the filename is the citation for the case from which the question is derived.  Thus, the just-mentioned file is derived from the case *Lanphere & Urbaniak v. State of Colorado*, 21 F.3d 1508 (10th Cir. 1994). 

The actual testing of LLMs against the dataset is done by running **step2_llms_analyze.py**.  This takes in the **.in.txt** files from **DATASET** and writes results to a corresponding **.out** file.  Then, you can use **analyze.py** to get detailed statistics from the **.out** files.  **RESULTS.txt** contains text that was output by **analyze.py**.  

This repository's code could be used to generate hard legal questions beyond just our 500.  First, one must download raw case files using **download.py**, which is currently set to download from the Federal Reporter, Third (F.3d), but could be set to download from F., F.2d, or F.4d as well.  Then, run **step1_generate_summaries.py** against o1 (or another model) to generate raw .txt summaries in the DATASET directory.  Next, run **step1.5_prepare_in.py**, which does not call any LLMs, to get the raw summaries into the expected format, which ends in **.in.txt**.  
